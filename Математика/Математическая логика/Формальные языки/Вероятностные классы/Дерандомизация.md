==Опр.== **Дерандомизация** - процесс удаления *(или минимизация использования)* случайности из [[Вероятностная машина Тьюринга (ВМТ)|вероятностного алгоритма]].

# Метод условных вероятностей
## Описание метода

Метод условных вероятностей заключается в том, что на каждом этапе удаления случайности мы не уменьшаем вероятность ошибки.

1) Пространство событий делится на две части (по свойствам задачи)
2) Для каждой части вычисляется условная вероятность ошибки
3) Пространство событий сужается на ту часть, где вероятность ошибки меньше
4) Сужение пространства заканчивается в тот момент, когда в нём останется одна единственная точка
5) Поскольку части пространства являются противоположными событиями, то сумма вероятностей этих событий равна 1, а при выборе "лучшей" части, мы не увеличиваем вероятность ошибки

## Алгоритм применения

1) Пусть ВП имеет вид $(B^n, 2^{B^n}, P)$ и является классическим ДВП. Пусть ЭИ имеет вид $(a_1, \dots, a_n)$
2) Пусть $A_i = \{a_i = 1\}$, пусть $X$ - ДСВ, описывающая целевую функцию задачи (например, количество выполненных дизъюнктов в задаче $SAT$)
3) В силу свойств [[Математическое ожидание и дисперсия стохастических СВ|матожидания]]: $\mathbb{E}X = \frac{1}{2}(\mathbb{E}(X|A_i) + \mathbb{E}(X|\overline{A_i}))$
4) И если мы выберем оптимальный вариант (например, если хотим максимизировать $X$, то выбираем большее по вероятности из двух событий), то боо $\mathbb{E}(X|A_i) \geq$ $\mathbb{E}(X|\overline{A_i})$ $\geq \mathbb{E}X$
5) Если сузить пространство $n$ раз, то в итоге получим набор $(a_1, \dots, a_n)$, при котором значение целевой функции будет не хуже, чем в рандомизированном алгоритме
